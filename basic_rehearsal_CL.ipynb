{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define dataset path\n",
    "# data_dir = 'Images_Dataset/train'\n",
    "data_dir = 'cards_dataset/train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "train_dir = 'cards_dataset/train'\n",
    "test_dir = 'cards_dataset/test'\n",
    "rehearsal_dir = 'cards_dataset/rehearsal'  # Unified directory for rehearsal buffer\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "\n",
    "# Set number of workers for data loading\n",
    "num_workers = 4\n",
    "\n",
    "# Load data with multi-threaded data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Load pre-trained EfficientNet model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Modify the classifier layer to match the number of classes in our dataset\n",
    "num_classes = len(train_dataset.classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RehearsalBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def add_data(self, data):\n",
    "        self.buffer.extend(data)\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer = random.sample(self.buffer, self.buffer_size)\n",
    "    \n",
    "    def sample_data(self, sample_size):\n",
    "        return random.sample(self.buffer, min(sample_size, len(self.buffer)))\n",
    "\n",
    "# Initialize rehearsal buffer\n",
    "buffer_size = 500  # Set buffer size\n",
    "rehearsal_buffer = RehearsalBuffer(buffer_size)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.000707\n",
    "\n",
    "sample_size = 64  # Number of samples to draw from the rehearsal buffer\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use AdamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7892953270978956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.11046772376151688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.06481805074039891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.04194758521429409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0335036436381963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.027492177195802152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.034034127407393096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.02179057022671675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.016975082291559463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.026243886142107938\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     # Wrap train_loader with tqdm for the progress bar\n",
    "#     train_loader_with_progress = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    \n",
    "#     for images, labels in train_loader_with_progress:\n",
    "#         # Add current batch to the rehearsal buffer\n",
    "#         rehearsal_buffer.add_data(list(zip(images, labels)))\n",
    "        \n",
    "#         # Sample from the rehearsal buffer\n",
    "#         if len(rehearsal_buffer.buffer) > 0:\n",
    "#             buffer_samples = rehearsal_buffer.sample_data(sample_size)\n",
    "#             buffer_images, buffer_labels = zip(*buffer_samples)\n",
    "#             buffer_images = torch.stack(buffer_images)\n",
    "#             buffer_labels = torch.tensor(buffer_labels)\n",
    "            \n",
    "#             # Combine current batch and buffer samples\n",
    "#             combined_images = torch.cat((images, buffer_images))\n",
    "#             combined_labels = torch.cat((labels, buffer_labels))\n",
    "#         else:\n",
    "#             combined_images = images\n",
    "#             combined_labels = labels\n",
    "        \n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(combined_images)\n",
    "#         loss = criterion(outputs, combined_labels)\n",
    "        \n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         # Update the progress bar with the current loss\n",
    "#         train_loader_with_progress.set_postfix({'Loss': loss.item()})\n",
    "    \n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to efficientnet_cl_model.pth\n"
     ]
    }
   ],
   "source": [
    "# # Define the path where you want to save the model\n",
    "# model_save_path = 'efficientnet_cl_model.pth'\n",
    "\n",
    "# # Save the model state dictionary\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Model loaded from efficientnet_cl_model_ORIG.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the path from where you want to load the CL model\n",
    "model_load_path = 'efficientnet_cl_model_ORIG.pth'\n",
    "\n",
    "# Load the EfficientNet model architecture\n",
    "model = models.efficientnet_b0(pretrained=False)  # Set pretrained=False to load the architecture without pretrained weights\n",
    "\n",
    "# Modify the classifier layer to match the number of classes in your dataset\n",
    "# (Make sure to adjust this based on your specific dataset)\n",
    "num_classes = len(train_dataset.classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load the state dictionary from the file specified by model_load_path into the model\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(f'CL Model loaded from {model_load_path}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 3.272138623520732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.8233898878097534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.3235774300992489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.28990335389971733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.2553742378950119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.706519216299057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 1.0843069851398468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.6572707891464233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.4093872085213661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.3289683163166046\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "test_dir = 'cards_dataset/test'\n",
    "rehearsal_dir = 'cards_dataset/rehearsal'  # Unified directory for rehearsal buffer\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "\n",
    "# Function to load rehearsal data from a directory\n",
    "def load_rehearsal_data(root_dir, transform):\n",
    "    if not os.listdir(root_dir):  # Check if the directory is empty\n",
    "        return None\n",
    "    return datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# Load the rehearsal dataset\n",
    "rehearsal_dataset = load_rehearsal_data(rehearsal_dir, data_transforms)\n",
    "\n",
    "# Split the testing dataset into fine-tuning and evaluation subsets\n",
    "fine_tune_size = 0.2  # Use 20% for fine-tuning\n",
    "fine_tune_indices, eval_indices = train_test_split(list(range(len(test_dataset))), test_size=fine_tune_size)\n",
    "fine_tune_subset = Subset(test_dataset, fine_tune_indices)\n",
    "eval_subset = Subset(test_dataset, eval_indices)\n",
    "\n",
    "# Create data loaders for fine-tuning and evaluation subsets\n",
    "fine_tune_loader = DataLoader(fine_tune_subset, batch_size=64, shuffle=True, num_workers=4)\n",
    "eval_loader = DataLoader(eval_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Set up fine-tuning parameters\n",
    "fine_tune_epochs = 5\n",
    "fine_tune_learning_rate = 0.00045\n",
    "fine_tune_optimizer = torch.optim.AdamW(model.parameters(), lr=fine_tune_learning_rate)\n",
    "fine_tune_criterion = torch.nn.CrossEntropyLoss()\n",
    "fine_tune_scheduler = ReduceLROnPlateau(fine_tune_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "class RehearsalBuffer:\n",
    "    def __init__(self, rehearsal_dataset):\n",
    "        self.buffer = []\n",
    "        self.rehearsal_dataset = rehearsal_dataset\n",
    "\n",
    "    def add_random_data(self, sample_per_class, sample_per_new_class):\n",
    "        class_indices = {}\n",
    "        \n",
    "        # Collect indices of each class in rehearsal_dataset\n",
    "        for i, (_, label) in enumerate(self.rehearsal_dataset):\n",
    "            if label not in class_indices:\n",
    "                class_indices[label] = []\n",
    "            class_indices[label].append(i)\n",
    "        \n",
    "        # Shuffle and select random samples from each class in rehearsal_dataset\n",
    "        for class_index, indices in class_indices.items():\n",
    "            random.shuffle(indices)\n",
    "            if class_index < 37:\n",
    "                selected_indices = indices[:sample_per_class]\n",
    "            else:\n",
    "                selected_indices = indices[:sample_per_new_class]\n",
    "            self.buffer.extend([self.rehearsal_dataset[i] for i in selected_indices])\n",
    "\n",
    "    def sample_data(self, sample_size):\n",
    "        sample_size = min(sample_size, len(self.buffer))\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "\n",
    "# Define the number of samples per class\n",
    "sample_per_class = 10\n",
    "sample_per_new_class = 110\n",
    "\n",
    "for i in range(0, 2):\n",
    "    # Load the updated rehearsal dataset within the loop\n",
    "    rehearsal_dataset = load_rehearsal_data(rehearsal_dir, data_transforms)\n",
    "    \n",
    "    # Initialize the rehearsal buffer with the updated datasets\n",
    "    rehearsal_buffer = RehearsalBuffer(rehearsal_dataset)\n",
    "\n",
    "    # Get the first new class\n",
    "    first_new_class = test_dataset.classes[i]\n",
    "\n",
    "    # Function to update the model for new classes\n",
    "    def add_new_classes_to_model(model, new_classes_count):\n",
    "        old_num_classes = model.classifier[1].out_features\n",
    "        new_num_classes = old_num_classes + new_classes_count\n",
    "        \n",
    "        # Get the weight and bias of the old classifier layer\n",
    "        old_weight = model.classifier[1].weight.data\n",
    "        old_bias = model.classifier[1].bias.data\n",
    "        \n",
    "        # Create a new classifier layer with updated number of classes\n",
    "        new_classifier = torch.nn.Linear(model.classifier[1].in_features, new_num_classes)\n",
    "        \n",
    "        # Initialize the new classifier layer with old weights and biases\n",
    "        new_classifier.weight.data[:old_num_classes] = old_weight\n",
    "        new_classifier.bias.data[:old_num_classes] = old_bias\n",
    "        \n",
    "        # Replace the old classifier with the new one\n",
    "        model.classifier[1] = new_classifier\n",
    "\n",
    "    # Add new class to the model before starting the training loop\n",
    "    add_new_classes_to_model(model, 1)  # Increment by one class\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Load data for the new class\n",
    "        class_index = test_dataset.class_to_idx[first_new_class]\n",
    "        class_indices = [i for i, (_, label) in enumerate(test_dataset) if label == class_index]\n",
    "        new_class_subset = Subset(test_dataset, class_indices)\n",
    "        new_class_loader = DataLoader(new_class_subset, batch_size=64, shuffle=True, num_workers=4)\n",
    "        \n",
    "        # Wrap the data loader with tqdm for the progress bar\n",
    "        new_class_loader_with_progress = tqdm(new_class_loader, desc=f'Epoch {epoch + 1}/{fine_tune_epochs} - Class {first_new_class}', leave=False)\n",
    "        \n",
    "        for images, labels in new_class_loader_with_progress:\n",
    "            # Adjust labels for the new combined classifier\n",
    "            labels += 37  # Shift labels to the correct position\n",
    "            \n",
    "            # Add samples from each class to the rehearsal buffer\n",
    "            rehearsal_buffer.add_random_data(sample_per_class, sample_per_new_class)\n",
    "            \n",
    "            # Sample from the rehearsal buffer\n",
    "            if len(rehearsal_buffer.buffer) > 0:\n",
    "                buffer_samples = rehearsal_buffer.sample_data(sample_per_class + sample_per_new_class)\n",
    "                buffer_images, buffer_labels = zip(*buffer_samples)\n",
    "                buffer_images = torch.stack(buffer_images)\n",
    "                buffer_labels = torch.tensor(buffer_labels)\n",
    "                \n",
    "                # Combine current batch and buffer samples\n",
    "                combined_images = torch.cat((images, buffer_images))\n",
    "                combined_labels = torch.cat((labels, buffer_labels))\n",
    "            else:\n",
    "                combined_images = images\n",
    "                combined_labels = labels\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            fine_tune_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(combined_images)\n",
    "            loss = fine_tune_criterion(outputs, combined_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            fine_tune_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Reduce learning rate if validation loss plateaus\n",
    "        fine_tune_scheduler.step(running_loss)\n",
    "        \n",
    "        # Print loss at the end of each epoch\n",
    "        print(f\"Epoch {epoch + 1}/{fine_tune_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "    # Copy the new class images to the rehearsal folder at the end of training\n",
    "\n",
    "    def copy_new_class_to_rehearsal(test_dir, rehearsal_dir, new_class_name, class_index):\n",
    "        new_class_dir = os.path.join(test_dir, new_class_name)\n",
    "        rehearsal_class_dir = os.path.join(rehearsal_dir, new_class_name)\n",
    "        \n",
    "        if not os.path.exists(rehearsal_class_dir):\n",
    "            os.makedirs(rehearsal_class_dir)\n",
    "        \n",
    "        for filename in os.listdir(new_class_dir):\n",
    "            source_path = os.path.join(new_class_dir, filename)\n",
    "            destination_path = os.path.join(rehearsal_class_dir, filename)\n",
    "            shutil.copy(source_path, destination_path)\n",
    "        \n",
    "        # Rename the rehearsal class folder to include the class index\n",
    "        new_rehearsal_class_dir = os.path.join(rehearsal_dir, f\"{class_index:02d}_{new_class_name}\")\n",
    "        os.rename(rehearsal_class_dir, new_rehearsal_class_dir)\n",
    "\n",
    "\n",
    "    # Execute the copy function\n",
    "    copy_new_class_to_rehearsal(test_dir, rehearsal_dir, first_new_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train dataset: 99.57%\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(root='cards_dataset/train', transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "train_accuracy = total_correct / total_samples\n",
    "print(f\"Accuracy on train dataset: {train_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 'ace of diamonds' dataset: 98.45%\n",
      "Average loss on 'ace of diamonds' dataset: 10.1119\n",
      "Predictions on 'ace of diamonds' dataset:\n",
      "Sample 0: Predicted class 37, Actual class 37\n",
      "Sample 1: Predicted class 37, Actual class 37\n",
      "Sample 2: Predicted class 37, Actual class 37\n",
      "Sample 3: Predicted class 37, Actual class 37\n",
      "Sample 4: Predicted class 37, Actual class 37\n",
      "Sample 5: Predicted class 37, Actual class 37\n",
      "Sample 6: Predicted class 37, Actual class 37\n",
      "Sample 7: Predicted class 37, Actual class 37\n",
      "Sample 8: Predicted class 37, Actual class 37\n",
      "Sample 9: Predicted class 37, Actual class 37\n",
      "Sample 10: Predicted class 37, Actual class 37\n",
      "Sample 11: Predicted class 37, Actual class 37\n",
      "Sample 12: Predicted class 37, Actual class 37\n",
      "Sample 13: Predicted class 37, Actual class 37\n",
      "Sample 14: Predicted class 37, Actual class 37\n",
      "Sample 15: Predicted class 37, Actual class 37\n",
      "Sample 16: Predicted class 37, Actual class 37\n",
      "Sample 17: Predicted class 37, Actual class 37\n",
      "Sample 18: Predicted class 37, Actual class 37\n",
      "Sample 19: Predicted class 37, Actual class 37\n",
      "Sample 20: Predicted class 37, Actual class 37\n",
      "Sample 21: Predicted class 37, Actual class 37\n",
      "Sample 22: Predicted class 37, Actual class 37\n",
      "Sample 23: Predicted class 37, Actual class 37\n",
      "Sample 24: Predicted class 37, Actual class 37\n",
      "Sample 25: Predicted class 37, Actual class 37\n",
      "Sample 26: Predicted class 37, Actual class 37\n",
      "Sample 27: Predicted class 37, Actual class 37\n",
      "Sample 28: Predicted class 37, Actual class 37\n",
      "Sample 29: Predicted class 37, Actual class 37\n",
      "Sample 30: Predicted class 37, Actual class 37\n",
      "Sample 31: Predicted class 37, Actual class 37\n",
      "Sample 32: Predicted class 37, Actual class 37\n",
      "Sample 33: Predicted class 37, Actual class 37\n",
      "Sample 34: Predicted class 37, Actual class 37\n",
      "Sample 35: Predicted class 37, Actual class 37\n",
      "Sample 36: Predicted class 37, Actual class 37\n",
      "Sample 37: Predicted class 37, Actual class 37\n",
      "Sample 38: Predicted class 37, Actual class 37\n",
      "Sample 39: Predicted class 37, Actual class 37\n",
      "Sample 40: Predicted class 37, Actual class 37\n",
      "Sample 41: Predicted class 37, Actual class 37\n",
      "Sample 42: Predicted class 37, Actual class 37\n",
      "Sample 43: Predicted class 37, Actual class 37\n",
      "Sample 44: Predicted class 37, Actual class 37\n",
      "Sample 45: Predicted class 37, Actual class 37\n",
      "Sample 46: Predicted class 37, Actual class 37\n",
      "Sample 47: Predicted class 37, Actual class 37\n",
      "Sample 48: Predicted class 37, Actual class 37\n",
      "Sample 49: Predicted class 37, Actual class 37\n",
      "Sample 50: Predicted class 37, Actual class 37\n",
      "Sample 51: Predicted class 37, Actual class 37\n",
      "Sample 52: Predicted class 37, Actual class 37\n",
      "Sample 53: Predicted class 37, Actual class 37\n",
      "Sample 54: Predicted class 37, Actual class 37\n",
      "Sample 55: Predicted class 37, Actual class 37\n",
      "Sample 56: Predicted class 37, Actual class 37\n",
      "Sample 57: Predicted class 37, Actual class 37\n",
      "Sample 58: Predicted class 37, Actual class 37\n",
      "Sample 59: Predicted class 37, Actual class 37\n",
      "Sample 60: Predicted class 37, Actual class 37\n",
      "Sample 61: Predicted class 37, Actual class 37\n",
      "Sample 62: Predicted class 37, Actual class 37\n",
      "Sample 63: Predicted class 37, Actual class 37\n",
      "Sample 64: Predicted class 37, Actual class 37\n",
      "Sample 65: Predicted class 37, Actual class 37\n",
      "Sample 66: Predicted class 37, Actual class 37\n",
      "Sample 67: Predicted class 37, Actual class 37\n",
      "Sample 68: Predicted class 37, Actual class 37\n",
      "Sample 69: Predicted class 37, Actual class 37\n",
      "Sample 70: Predicted class 37, Actual class 37\n",
      "Sample 71: Predicted class 37, Actual class 37\n",
      "Sample 72: Predicted class 37, Actual class 37\n",
      "Sample 73: Predicted class 37, Actual class 37\n",
      "Sample 74: Predicted class 37, Actual class 37\n",
      "Sample 75: Predicted class 37, Actual class 37\n",
      "Sample 76: Predicted class 37, Actual class 37\n",
      "Sample 77: Predicted class 37, Actual class 37\n",
      "Sample 78: Predicted class 37, Actual class 37\n",
      "Sample 79: Predicted class 37, Actual class 37\n",
      "Sample 80: Predicted class 37, Actual class 37\n",
      "Sample 81: Predicted class 37, Actual class 37\n",
      "Sample 82: Predicted class 37, Actual class 37\n",
      "Sample 83: Predicted class 37, Actual class 37\n",
      "Sample 84: Predicted class 37, Actual class 37\n",
      "Sample 85: Predicted class 37, Actual class 37\n",
      "Sample 86: Predicted class 37, Actual class 37\n",
      "Sample 87: Predicted class 37, Actual class 37\n",
      "Sample 88: Predicted class 37, Actual class 37\n",
      "Sample 89: Predicted class 37, Actual class 37\n",
      "Sample 90: Predicted class 37, Actual class 37\n",
      "Sample 91: Predicted class 37, Actual class 37\n",
      "Sample 92: Predicted class 37, Actual class 37\n",
      "Sample 93: Predicted class 37, Actual class 37\n",
      "Sample 94: Predicted class 37, Actual class 37\n",
      "Sample 95: Predicted class 37, Actual class 37\n",
      "Sample 96: Predicted class 37, Actual class 37\n",
      "Sample 97: Predicted class 37, Actual class 37\n",
      "Sample 98: Predicted class 37, Actual class 37\n",
      "Sample 99: Predicted class 37, Actual class 37\n",
      "Sample 100: Predicted class 37, Actual class 37\n",
      "Sample 101: Predicted class 37, Actual class 37\n",
      "Sample 102: Predicted class 38, Actual class 37\n",
      "Sample 103: Predicted class 37, Actual class 37\n",
      "Sample 104: Predicted class 37, Actual class 37\n",
      "Sample 105: Predicted class 37, Actual class 37\n",
      "Sample 106: Predicted class 37, Actual class 37\n",
      "Sample 107: Predicted class 37, Actual class 37\n",
      "Sample 108: Predicted class 37, Actual class 37\n",
      "Sample 109: Predicted class 37, Actual class 37\n",
      "Sample 110: Predicted class 37, Actual class 37\n",
      "Sample 111: Predicted class 37, Actual class 37\n",
      "Sample 112: Predicted class 37, Actual class 37\n",
      "Sample 113: Predicted class 37, Actual class 37\n",
      "Sample 114: Predicted class 37, Actual class 37\n",
      "Sample 115: Predicted class 37, Actual class 37\n",
      "Sample 116: Predicted class 37, Actual class 37\n",
      "Sample 117: Predicted class 37, Actual class 37\n",
      "Sample 118: Predicted class 37, Actual class 37\n",
      "Sample 119: Predicted class 37, Actual class 37\n",
      "Sample 120: Predicted class 37, Actual class 37\n",
      "Sample 121: Predicted class 37, Actual class 37\n",
      "Sample 122: Predicted class 37, Actual class 37\n",
      "Sample 123: Predicted class 38, Actual class 37\n",
      "Sample 124: Predicted class 37, Actual class 37\n",
      "Sample 125: Predicted class 37, Actual class 37\n",
      "Sample 126: Predicted class 37, Actual class 37\n",
      "Sample 127: Predicted class 37, Actual class 37\n",
      "Sample 128: Predicted class 37, Actual class 37\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "train_dir = 'cards_dataset/test'\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "\n",
    "# Get the class index for \"\"\n",
    "class_name = \"ace of diamonds\"\n",
    "class_idx = train_dataset.class_to_idx[class_name]\n",
    "\n",
    "# Filter indices for the \"two of spades\" class\n",
    "class_indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_idx]\n",
    "\n",
    "# Create a subset and DataLoader for the \"two of spades\" class\n",
    "class_subset = Subset(train_dataset, class_indices)\n",
    "class_loader = DataLoader(class_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == 37).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss, all_predictions, all_labels\n",
    "\n",
    "# Evaluate the model on the \"ace of diamonds\" subset\n",
    "accuracy_class, avg_loss_class, predictions_class, actuals_class = evaluate_model(model, class_loader, fine_tune_criterion)\n",
    "\n",
    "print(f\"Accuracy on 'ace of diamonds' dataset: {accuracy_class:.2f}%\")\n",
    "print(f\"Average loss on 'ace of diamonds' dataset: {avg_loss_class:.4f}\")\n",
    "\n",
    "# Print predictions along with actual labels for the \"two of spades\" dataset\n",
    "print(\"Predictions on 'ace of diamonds' dataset:\")\n",
    "for idx, (pred, actual) in enumerate(zip(predictions_class, actuals_class)):\n",
    "    print(f\"Sample {idx}: Predicted class {pred}, Actual class {actual + 37}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 'ace of hearts' dataset: 100.00%\n",
      "Average loss on 'ace of hearts' dataset: 17.6494\n",
      "Predictions on 'ace of hearts' dataset:\n",
      "Sample 0: Predicted class 38, Actual class 38\n",
      "Sample 1: Predicted class 38, Actual class 38\n",
      "Sample 2: Predicted class 38, Actual class 38\n",
      "Sample 3: Predicted class 38, Actual class 38\n",
      "Sample 4: Predicted class 38, Actual class 38\n",
      "Sample 5: Predicted class 38, Actual class 38\n",
      "Sample 6: Predicted class 38, Actual class 38\n",
      "Sample 7: Predicted class 38, Actual class 38\n",
      "Sample 8: Predicted class 38, Actual class 38\n",
      "Sample 9: Predicted class 38, Actual class 38\n",
      "Sample 10: Predicted class 38, Actual class 38\n",
      "Sample 11: Predicted class 38, Actual class 38\n",
      "Sample 12: Predicted class 38, Actual class 38\n",
      "Sample 13: Predicted class 38, Actual class 38\n",
      "Sample 14: Predicted class 38, Actual class 38\n",
      "Sample 15: Predicted class 38, Actual class 38\n",
      "Sample 16: Predicted class 38, Actual class 38\n",
      "Sample 17: Predicted class 38, Actual class 38\n",
      "Sample 18: Predicted class 38, Actual class 38\n",
      "Sample 19: Predicted class 38, Actual class 38\n",
      "Sample 20: Predicted class 38, Actual class 38\n",
      "Sample 21: Predicted class 38, Actual class 38\n",
      "Sample 22: Predicted class 38, Actual class 38\n",
      "Sample 23: Predicted class 38, Actual class 38\n",
      "Sample 24: Predicted class 38, Actual class 38\n",
      "Sample 25: Predicted class 38, Actual class 38\n",
      "Sample 26: Predicted class 38, Actual class 38\n",
      "Sample 27: Predicted class 38, Actual class 38\n",
      "Sample 28: Predicted class 38, Actual class 38\n",
      "Sample 29: Predicted class 38, Actual class 38\n",
      "Sample 30: Predicted class 38, Actual class 38\n",
      "Sample 31: Predicted class 38, Actual class 38\n",
      "Sample 32: Predicted class 38, Actual class 38\n",
      "Sample 33: Predicted class 38, Actual class 38\n",
      "Sample 34: Predicted class 38, Actual class 38\n",
      "Sample 35: Predicted class 38, Actual class 38\n",
      "Sample 36: Predicted class 38, Actual class 38\n",
      "Sample 37: Predicted class 38, Actual class 38\n",
      "Sample 38: Predicted class 38, Actual class 38\n",
      "Sample 39: Predicted class 38, Actual class 38\n",
      "Sample 40: Predicted class 38, Actual class 38\n",
      "Sample 41: Predicted class 38, Actual class 38\n",
      "Sample 42: Predicted class 38, Actual class 38\n",
      "Sample 43: Predicted class 38, Actual class 38\n",
      "Sample 44: Predicted class 38, Actual class 38\n",
      "Sample 45: Predicted class 38, Actual class 38\n",
      "Sample 46: Predicted class 38, Actual class 38\n",
      "Sample 47: Predicted class 38, Actual class 38\n",
      "Sample 48: Predicted class 38, Actual class 38\n",
      "Sample 49: Predicted class 38, Actual class 38\n",
      "Sample 50: Predicted class 38, Actual class 38\n",
      "Sample 51: Predicted class 38, Actual class 38\n",
      "Sample 52: Predicted class 38, Actual class 38\n",
      "Sample 53: Predicted class 38, Actual class 38\n",
      "Sample 54: Predicted class 38, Actual class 38\n",
      "Sample 55: Predicted class 38, Actual class 38\n",
      "Sample 56: Predicted class 38, Actual class 38\n",
      "Sample 57: Predicted class 38, Actual class 38\n",
      "Sample 58: Predicted class 38, Actual class 38\n",
      "Sample 59: Predicted class 38, Actual class 38\n",
      "Sample 60: Predicted class 38, Actual class 38\n",
      "Sample 61: Predicted class 38, Actual class 38\n",
      "Sample 62: Predicted class 38, Actual class 38\n",
      "Sample 63: Predicted class 38, Actual class 38\n",
      "Sample 64: Predicted class 38, Actual class 38\n",
      "Sample 65: Predicted class 38, Actual class 38\n",
      "Sample 66: Predicted class 38, Actual class 38\n",
      "Sample 67: Predicted class 38, Actual class 38\n",
      "Sample 68: Predicted class 38, Actual class 38\n",
      "Sample 69: Predicted class 38, Actual class 38\n",
      "Sample 70: Predicted class 38, Actual class 38\n",
      "Sample 71: Predicted class 38, Actual class 38\n",
      "Sample 72: Predicted class 38, Actual class 38\n",
      "Sample 73: Predicted class 38, Actual class 38\n",
      "Sample 74: Predicted class 38, Actual class 38\n",
      "Sample 75: Predicted class 38, Actual class 38\n",
      "Sample 76: Predicted class 38, Actual class 38\n",
      "Sample 77: Predicted class 38, Actual class 38\n",
      "Sample 78: Predicted class 38, Actual class 38\n",
      "Sample 79: Predicted class 38, Actual class 38\n",
      "Sample 80: Predicted class 38, Actual class 38\n",
      "Sample 81: Predicted class 38, Actual class 38\n",
      "Sample 82: Predicted class 38, Actual class 38\n",
      "Sample 83: Predicted class 38, Actual class 38\n",
      "Sample 84: Predicted class 38, Actual class 38\n",
      "Sample 85: Predicted class 38, Actual class 38\n",
      "Sample 86: Predicted class 38, Actual class 38\n",
      "Sample 87: Predicted class 38, Actual class 38\n",
      "Sample 88: Predicted class 38, Actual class 38\n",
      "Sample 89: Predicted class 38, Actual class 38\n",
      "Sample 90: Predicted class 38, Actual class 38\n",
      "Sample 91: Predicted class 38, Actual class 38\n",
      "Sample 92: Predicted class 38, Actual class 38\n",
      "Sample 93: Predicted class 38, Actual class 38\n",
      "Sample 94: Predicted class 38, Actual class 38\n",
      "Sample 95: Predicted class 38, Actual class 38\n",
      "Sample 96: Predicted class 38, Actual class 38\n",
      "Sample 97: Predicted class 38, Actual class 38\n",
      "Sample 98: Predicted class 38, Actual class 38\n",
      "Sample 99: Predicted class 38, Actual class 38\n",
      "Sample 100: Predicted class 38, Actual class 38\n",
      "Sample 101: Predicted class 38, Actual class 38\n",
      "Sample 102: Predicted class 38, Actual class 38\n",
      "Sample 103: Predicted class 38, Actual class 38\n",
      "Sample 104: Predicted class 38, Actual class 38\n",
      "Sample 105: Predicted class 38, Actual class 38\n",
      "Sample 106: Predicted class 38, Actual class 38\n",
      "Sample 107: Predicted class 38, Actual class 38\n",
      "Sample 108: Predicted class 38, Actual class 38\n",
      "Sample 109: Predicted class 38, Actual class 38\n",
      "Sample 110: Predicted class 38, Actual class 38\n",
      "Sample 111: Predicted class 38, Actual class 38\n",
      "Sample 112: Predicted class 38, Actual class 38\n",
      "Sample 113: Predicted class 38, Actual class 38\n",
      "Sample 114: Predicted class 38, Actual class 38\n",
      "Sample 115: Predicted class 38, Actual class 38\n",
      "Sample 116: Predicted class 38, Actual class 38\n",
      "Sample 117: Predicted class 38, Actual class 38\n",
      "Sample 118: Predicted class 38, Actual class 38\n",
      "Sample 119: Predicted class 38, Actual class 38\n",
      "Sample 120: Predicted class 38, Actual class 38\n",
      "Sample 121: Predicted class 38, Actual class 38\n",
      "Sample 122: Predicted class 38, Actual class 38\n",
      "Sample 123: Predicted class 38, Actual class 38\n",
      "Sample 124: Predicted class 38, Actual class 38\n",
      "Sample 125: Predicted class 38, Actual class 38\n",
      "Sample 126: Predicted class 38, Actual class 38\n",
      "Sample 127: Predicted class 38, Actual class 38\n",
      "Sample 128: Predicted class 38, Actual class 38\n",
      "Sample 129: Predicted class 38, Actual class 38\n",
      "Sample 130: Predicted class 38, Actual class 38\n",
      "Sample 131: Predicted class 38, Actual class 38\n",
      "Sample 132: Predicted class 38, Actual class 38\n",
      "Sample 133: Predicted class 38, Actual class 38\n",
      "Sample 134: Predicted class 38, Actual class 38\n",
      "Sample 135: Predicted class 38, Actual class 38\n",
      "Sample 136: Predicted class 38, Actual class 38\n",
      "Sample 137: Predicted class 38, Actual class 38\n",
      "Sample 138: Predicted class 38, Actual class 38\n",
      "Sample 139: Predicted class 38, Actual class 38\n",
      "Sample 140: Predicted class 38, Actual class 38\n",
      "Sample 141: Predicted class 38, Actual class 38\n",
      "Sample 142: Predicted class 38, Actual class 38\n",
      "Sample 143: Predicted class 38, Actual class 38\n",
      "Sample 144: Predicted class 38, Actual class 38\n",
      "Sample 145: Predicted class 38, Actual class 38\n",
      "Sample 146: Predicted class 38, Actual class 38\n",
      "Sample 147: Predicted class 38, Actual class 38\n",
      "Sample 148: Predicted class 38, Actual class 38\n",
      "Sample 149: Predicted class 38, Actual class 38\n",
      "Sample 150: Predicted class 38, Actual class 38\n",
      "Sample 151: Predicted class 38, Actual class 38\n",
      "Sample 152: Predicted class 38, Actual class 38\n",
      "Sample 153: Predicted class 38, Actual class 38\n",
      "Sample 154: Predicted class 38, Actual class 38\n",
      "Sample 155: Predicted class 38, Actual class 38\n",
      "Sample 156: Predicted class 38, Actual class 38\n",
      "Sample 157: Predicted class 38, Actual class 38\n",
      "Sample 158: Predicted class 38, Actual class 38\n",
      "Sample 159: Predicted class 38, Actual class 38\n",
      "Sample 160: Predicted class 38, Actual class 38\n",
      "Sample 161: Predicted class 38, Actual class 38\n",
      "Sample 162: Predicted class 38, Actual class 38\n",
      "Sample 163: Predicted class 38, Actual class 38\n",
      "Sample 164: Predicted class 38, Actual class 38\n",
      "Sample 165: Predicted class 38, Actual class 38\n",
      "Sample 166: Predicted class 38, Actual class 38\n",
      "Sample 167: Predicted class 38, Actual class 38\n",
      "Sample 168: Predicted class 38, Actual class 38\n",
      "Sample 169: Predicted class 38, Actual class 38\n",
      "Sample 170: Predicted class 38, Actual class 38\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "train_dir = 'cards_dataset/test'\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "\n",
    "# Get the class index for \"\"\n",
    "class_name = \"ace of hearts\"\n",
    "class_idx = train_dataset.class_to_idx[class_name]\n",
    "\n",
    "# Filter indices for the \"two of spades\" class\n",
    "class_indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_idx]\n",
    "\n",
    "# Create a subset and DataLoader for the \"two of spades\" class\n",
    "class_subset = Subset(train_dataset, class_indices)\n",
    "class_loader = DataLoader(class_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == 38).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss, all_predictions, all_labels\n",
    "\n",
    "# Evaluate the model on the \"ace of diamonds\" subset\n",
    "accuracy_class, avg_loss_class, predictions_class, actuals_class = evaluate_model(model, class_loader, fine_tune_criterion)\n",
    "\n",
    "print(f\"Accuracy on 'ace of hearts' dataset: {accuracy_class:.2f}%\")\n",
    "print(f\"Average loss on 'ace of hearts' dataset: {avg_loss_class:.4f}\")\n",
    "\n",
    "# Print predictions along with actual labels for the \"two of spades\" dataset\n",
    "print(\"Predictions on 'ace of hearts' dataset:\")\n",
    "for idx, (pred, actual) in enumerate(zip(predictions_class, actuals_class)):\n",
    "    print(f\"Sample {idx}: Predicted class {pred}, Actual class {actual + 37}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
