{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define dataset path\n",
    "# data_dir = 'Images_Dataset/train'\n",
    "data_dir = 'cards_dataset/train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "train_dir = 'cards_dataset/train'\n",
    "test_dir = 'cards_dataset/test'\n",
    "rehearsal_dir = 'cards_dataset/rehearsal'  # Unified directory for rehearsal buffer\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "\n",
    "# Set number of workers for data loading\n",
    "num_workers = 4\n",
    "\n",
    "# Load data with multi-threaded data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Load pre-trained EfficientNet model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Modify the classifier layer to match the number of classes in our dataset\n",
    "num_classes = len(train_dataset.classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RehearsalBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def add_data(self, data):\n",
    "        self.buffer.extend(data)\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer = random.sample(self.buffer, self.buffer_size)\n",
    "    \n",
    "    def sample_data(self, sample_size):\n",
    "        return random.sample(self.buffer, min(sample_size, len(self.buffer)))\n",
    "\n",
    "# Initialize rehearsal buffer\n",
    "buffer_size = 500  # Set buffer size\n",
    "rehearsal_buffer = RehearsalBuffer(buffer_size)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.000707\n",
    "\n",
    "sample_size = 64  # Number of samples to draw from the rehearsal buffer\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use AdamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     # Wrap train_loader with tqdm for the progress bar\n",
    "#     train_loader_with_progress = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    \n",
    "#     for images, labels in train_loader_with_progress:\n",
    "#         # Add current batch to the rehearsal buffer\n",
    "#         rehearsal_buffer.add_data(list(zip(images, labels)))\n",
    "        \n",
    "#         # Sample from the rehearsal buffer\n",
    "#         if len(rehearsal_buffer.buffer) > 0:\n",
    "#             buffer_samples = rehearsal_buffer.sample_data(sample_size)\n",
    "#             buffer_images, buffer_labels = zip(*buffer_samples)\n",
    "#             buffer_images = torch.stack(buffer_images)\n",
    "#             buffer_labels = torch.tensor(buffer_labels)\n",
    "            \n",
    "#             # Combine current batch and buffer samples\n",
    "#             combined_images = torch.cat((images, buffer_images))\n",
    "#             combined_labels = torch.cat((labels, buffer_labels))\n",
    "#         else:\n",
    "#             combined_images = images\n",
    "#             combined_labels = labels\n",
    "        \n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(combined_images)\n",
    "#         loss = criterion(outputs, combined_labels)\n",
    "        \n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         # Update the progress bar with the current loss\n",
    "#         train_loader_with_progress.set_postfix({'Loss': loss.item()})\n",
    "    \n",
    "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where you want to save the model\n",
    "# model_save_path = 'efficientnet_cl_model.pth'\n",
    "\n",
    "# # Save the model state dictionary\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Model loaded from efficientnet_cl_model_ORIG.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the path from where you want to load the CL model\n",
    "model_load_path = 'efficientnet_cl_model_ORIG.pth'\n",
    "\n",
    "# Load the EfficientNet model architecture\n",
    "model = models.efficientnet_b0(pretrained=False)  # Set pretrained=False to load the architecture without pretrained weights\n",
    "\n",
    "# Modify the classifier layer to match the number of classes in your dataset\n",
    "# (Make sure to adjust this based on your specific dataset)\n",
    "num_classes = len(train_dataset.classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load the state dictionary from the file specified by model_load_path into the model\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(f'CL Model loaded from {model_load_path}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved cards_dataset/rehearsal\\ace of diamonds to cards_dataset/ignore\\ace of diamonds\n",
      "Moved cards_dataset/rehearsal\\ace of hearts to cards_dataset/ignore\\ace of hearts\n",
      "Moved cards_dataset/rehearsal\\ace of spades to cards_dataset/ignore\\ace of spades\n",
      "Moved cards_dataset/rehearsal\\eight of clubs to cards_dataset/ignore\\eight of clubs\n",
      "Moved cards_dataset/rehearsal\\eight of diamonds to cards_dataset/ignore\\eight of diamonds\n",
      "Moved cards_dataset/rehearsal\\eight of hearts to cards_dataset/ignore\\eight of hearts\n",
      "Moved cards_dataset/rehearsal\\eight of spades to cards_dataset/ignore\\eight of spades\n",
      "Folders moved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source and destination directories\n",
    "rehearsal_dir = 'cards_dataset/rehearsal'\n",
    "ignore_dir = 'cards_dataset/ignore'\n",
    "\n",
    "# Create the ignore directory if it doesn't exist\n",
    "os.makedirs(ignore_dir, exist_ok=True)\n",
    "\n",
    "# Get all subdirectories in the rehearsal directory\n",
    "subdirs = [d for d in os.listdir(rehearsal_dir) if os.path.isdir(os.path.join(rehearsal_dir, d))]\n",
    "\n",
    "# Move subdirectories from index 37 onwards to the ignore directory\n",
    "for idx, subdir in enumerate(subdirs):\n",
    "    if idx >= 37:\n",
    "        src_path = os.path.join(rehearsal_dir, subdir)\n",
    "        dst_path = os.path.join(ignore_dir, subdir)\n",
    "        \n",
    "        # Remove existing destination directory if it exists\n",
    "        if os.path.exists(dst_path):\n",
    "            shutil.rmtree(dst_path)\n",
    "        \n",
    "        # Move the directory\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f\"Moved {src_path} to {dst_path}\")\n",
    "\n",
    "print(\"Folders moved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output neurons: 37\n"
     ]
    }
   ],
   "source": [
    "output_neurons = model.classifier[1].out_features\n",
    "print(f\"Number of output neurons: {output_neurons}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.314264327287674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 3.141158774495125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 1.4734115041792393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 1.1331035271286964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.8770162016153336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 4.063139200210571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 2.8599905967712402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 2.412838876247406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 1.8568164706230164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 1.1451730132102966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 4.575119495391846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 3.9439268112182617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 2.6603443026542664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 2.1106892228126526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 1.7627648711204529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.024605631828308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 4.029364168643951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 3.8887245655059814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 2.673683673143387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 1.7037035822868347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.349867343902588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 4.717755675315857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 3.0691598057746887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 2.271454691886902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 1.8089259266853333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.222139358520508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 4.261289834976196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 3.687012255191803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 2.4757957756519318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 1.389076054096222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.409898400306702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 4.727463722229004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 3.405801773071289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 2.390485107898712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 2.0645902156829834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "test_dir = 'cards_dataset/test'\n",
    "rehearsal_dir = 'cards_dataset/rehearsal'  # Unified directory for rehearsal buffer\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "\n",
    "# Function to load rehearsal data from a directory\n",
    "def load_rehearsal_data(root_dir, transform):\n",
    "    if not os.listdir(root_dir):  # Check if the directory is empty\n",
    "        return None\n",
    "    return datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# Load the rehearsal dataset\n",
    "rehearsal_dataset = load_rehearsal_data(rehearsal_dir, data_transforms)\n",
    "\n",
    "# Split the testing dataset into fine-tuning and evaluation subsets\n",
    "fine_tune_size = 0.1  # Use 10% for fine-tuning\n",
    "fine_tune_indices, eval_indices = train_test_split(list(range(len(test_dataset))), test_size=fine_tune_size)\n",
    "fine_tune_subset = Subset(test_dataset, fine_tune_indices)\n",
    "eval_subset = Subset(test_dataset, eval_indices)\n",
    "\n",
    "# Create data loaders for fine-tuning and evaluation subsets\n",
    "fine_tune_loader = DataLoader(fine_tune_subset, batch_size=64, shuffle=True, num_workers=4)\n",
    "eval_loader = DataLoader(eval_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Set up fine-tuning parameters\n",
    "fine_tune_epochs = 8\n",
    "fine_tune_learning_rate = 0.00045\n",
    "fine_tune_optimizer = torch.optim.AdamW(model.parameters(), lr=fine_tune_learning_rate)\n",
    "\n",
    "# Set up the CyclicLR scheduler\n",
    "base_lr = 0.0001\n",
    "max_lr = 0.001\n",
    "fine_tune_scheduler = CyclicLR(fine_tune_optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=5, mode='triangular')\n",
    "\n",
    "fine_tune_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class RehearsalBuffer:\n",
    "    def __init__(self, rehearsal_dataset):\n",
    "        self.buffer = []\n",
    "        self.rehearsal_dataset = rehearsal_dataset\n",
    "\n",
    "    def add_random_data(self, sample_per_class, sample_per_new_class):\n",
    "        class_indices = {}\n",
    "        \n",
    "        # Collect indices of each class in rehearsal_dataset\n",
    "        for i, (_, label) in enumerate(self.rehearsal_dataset):\n",
    "            if label not in class_indices:\n",
    "                class_indices[label] = []\n",
    "            class_indices[label].append(i)\n",
    "        \n",
    "        # Shuffle and select random samples from each class in rehearsal_dataset\n",
    "        for class_index, indices in class_indices.items():\n",
    "            random.shuffle(indices)\n",
    "            if class_index < 37:\n",
    "                selected_indices = indices[:sample_per_class]\n",
    "            else:\n",
    "                selected_indices = indices[:sample_per_new_class]\n",
    "            self.buffer.extend([self.rehearsal_dataset[i] for i in selected_indices])\n",
    "\n",
    "    def sample_data(self, sample_size):\n",
    "        sample_size = min(sample_size, len(self.buffer))\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "\n",
    "# Define the number of samples per class\n",
    "sample_per_class = 2\n",
    "sample_per_new_class = 10\n",
    "\n",
    "for i in range(0, 7):\n",
    "    # Load the updated rehearsal dataset within the loop\n",
    "    rehearsal_dataset = load_rehearsal_data(rehearsal_dir, data_transforms)\n",
    "    \n",
    "    # Initialize the rehearsal buffer with the updated datasets\n",
    "    rehearsal_buffer = RehearsalBuffer(rehearsal_dataset)\n",
    "\n",
    "    # Get the first new class\n",
    "    first_new_class = test_dataset.classes[i]\n",
    "\n",
    "    # Function to update the model for new classes\n",
    "    def add_new_classes_to_model(model, new_classes_count):\n",
    "        old_num_classes = model.classifier[1].out_features\n",
    "        new_num_classes = old_num_classes + new_classes_count\n",
    "        \n",
    "        # Get the weight and bias of the old classifier layer\n",
    "        old_weight = model.classifier[1].weight.data\n",
    "        old_bias = model.classifier[1].bias.data\n",
    "        \n",
    "        # Create a new classifier layer with updated number of classes\n",
    "        new_classifier = torch.nn.Linear(model.classifier[1].in_features, new_num_classes)\n",
    "        \n",
    "        # Initialize the new classifier layer with old weights and biases\n",
    "        new_classifier.weight.data[:old_num_classes] = old_weight\n",
    "        new_classifier.bias.data[:old_num_classes] = old_bias\n",
    "        \n",
    "        # Replace the old classifier with the new one\n",
    "        model.classifier[1] = new_classifier\n",
    "\n",
    "    # Add new class to the model before starting the training loop\n",
    "    add_new_classes_to_model(model, 1)  # Increment by one class\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Load data for the new class\n",
    "        class_index = test_dataset.class_to_idx[first_new_class]\n",
    "        class_indices = [i for i, (_, label) in enumerate(test_dataset) if label == class_index]\n",
    "        new_class_subset = Subset(test_dataset, class_indices)\n",
    "        new_class_loader = DataLoader(new_class_subset, batch_size=64, shuffle=True, num_workers=4)\n",
    "        \n",
    "        # Wrap the data loader with tqdm for the progress bar\n",
    "        new_class_loader_with_progress = tqdm(new_class_loader, desc=f'Epoch {epoch + 1}/{fine_tune_epochs} - Class {first_new_class}', leave=False)\n",
    "        \n",
    "        for images, labels in new_class_loader_with_progress:\n",
    "            # Adjust labels for the new combined classifier\n",
    "            labels += 37  # Shift labels to the correct position\n",
    "            \n",
    "            # Add samples from each class to the rehearsal buffer\n",
    "            rehearsal_buffer.add_random_data(sample_per_class, sample_per_new_class)\n",
    "            \n",
    "            # Sample from the rehearsal buffer\n",
    "            if len(rehearsal_buffer.buffer) > 0:\n",
    "                buffer_samples = rehearsal_buffer.sample_data(sample_per_class + sample_per_new_class)\n",
    "                buffer_images, buffer_labels = zip(*buffer_samples)\n",
    "                buffer_images = torch.stack(buffer_images)\n",
    "                buffer_labels = torch.tensor(buffer_labels)\n",
    "                \n",
    "                # Combine current batch and buffer samples\n",
    "                combined_images = torch.cat((images, buffer_images))\n",
    "                combined_labels = torch.cat((labels, buffer_labels))\n",
    "            else:\n",
    "                combined_images = images\n",
    "                combined_labels = labels\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            fine_tune_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(combined_images)\n",
    "            loss = fine_tune_criterion(outputs, combined_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            fine_tune_optimizer.step()\n",
    "\n",
    "            # Step the CyclicLR scheduler\n",
    "            fine_tune_scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print loss at the end of each epoch\n",
    "        print(f\"Epoch {epoch + 1}/{fine_tune_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "    # Copy the new class images to the rehearsal folder at the end of training\n",
    "\n",
    "    def copy_new_class_to_rehearsal(test_dir, rehearsal_dir, new_class_name, class_index):\n",
    "        new_class_dir = os.path.join(test_dir, new_class_name)\n",
    "        rehearsal_class_dir = os.path.join(rehearsal_dir, new_class_name)\n",
    "        \n",
    "        if not os.path.exists(rehearsal_class_dir):\n",
    "            os.makedirs(rehearsal_class_dir)\n",
    "        \n",
    "        for filename in os.listdir(new_class_dir):\n",
    "            source_path = os.path.join(new_class_dir, filename)\n",
    "            destination_path = os.path.join(rehearsal_class_dir, filename)\n",
    "            shutil.copy(source_path, destination_path)\n",
    "\n",
    "    # Execute the copy function\n",
    "    copy_new_class_to_rehearsal(test_dir, rehearsal_dir, first_new_class, i + 37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.434596538543701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 2.628715991973877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 1.0855591744184494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.7600512206554413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.46160848438739777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from torch.utils.data import Subset, DataLoader\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torchvision import datasets, transforms\n",
    "# import random\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the data transformations\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Directories for the dataset\n",
    "# test_dir = 'cards_dataset/test'\n",
    "# rehearsal_dir = 'cards_dataset/rehearsal'  # Unified directory for rehearsal buffer\n",
    "# test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "\n",
    "# # Function to load rehearsal data from a directory\n",
    "# def load_rehearsal_data(root_dir, transform):\n",
    "#     if not os.listdir(root_dir):  # Check if the directory is empty\n",
    "#         return None\n",
    "#     return datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# # Load the rehearsal dataset\n",
    "# rehearsal_dataset = load_rehearsal_data(rehearsal_dir, data_transforms)\n",
    "\n",
    "# # Split the testing dataset into fine-tuning and evaluation subsets\n",
    "# fine_tune_size = 0.1  # Use 20% for fine-tuning\n",
    "# fine_tune_indices, eval_indices = train_test_split(list(range(len(test_dataset))), test_size=fine_tune_size)\n",
    "# fine_tune_subset = Subset(test_dataset, fine_tune_indices)\n",
    "# eval_subset = Subset(test_dataset, eval_indices)\n",
    "\n",
    "# # Create data loaders for fine-tuning and evaluation subsets\n",
    "# fine_tune_loader = DataLoader(fine_tune_subset, batch_size=64, shuffle=True, num_workers=4)\n",
    "# eval_loader = DataLoader(eval_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# # Set up fine-tuning parameters\n",
    "# fine_tune_epochs = 5\n",
    "# fine_tune_learning_rate = 0.00045\n",
    "# fine_tune_optimizer = torch.optim.AdamW(model.parameters(), lr=fine_tune_learning_rate)\n",
    "# fine_tune_criterion = torch.nn.CrossEntropyLoss()\n",
    "# fine_tune_scheduler = ReduceLROnPlateau(fine_tune_optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# class RehearsalBuffer:\n",
    "#     def __init__(self, rehearsal_dataset):\n",
    "#         self.buffer = []\n",
    "#         self.rehearsal_dataset = rehearsal_dataset\n",
    "\n",
    "#     def add_random_data(self, sample_per_class, sample_per_new_class):\n",
    "#         class_indices = {}\n",
    "        \n",
    "#         # Collect indices of each class in rehearsal_dataset\n",
    "#         for i, (_, label) in enumerate(self.rehearsal_dataset):\n",
    "#             if label not in class_indices:\n",
    "#                 class_indices[label] = []\n",
    "#             class_indices[label].append(i)\n",
    "        \n",
    "#         # Shuffle and select random samples from each class in rehearsal_dataset\n",
    "#         for class_index, indices in class_indices.items():\n",
    "#             random.shuffle(indices)\n",
    "#             if class_index < 37:\n",
    "#                 selected_indices = indices[:sample_per_class]\n",
    "#             else:\n",
    "#                 selected_indices = indices[:sample_per_new_class]\n",
    "#             self.buffer.extend([self.rehearsal_dataset[i] for i in selected_indices])\n",
    "\n",
    "#     def sample_data(self, sample_size):\n",
    "#         sample_size = min(sample_size, len(self.buffer))\n",
    "#         return random.sample(self.buffer, sample_size)\n",
    "\n",
    "# # Define the number of samples per class\n",
    "# sample_per_class = 2\n",
    "# sample_per_new_class = 44\n",
    "\n",
    "# for i in range(0, 3):\n",
    "#     # Load the updated rehearsal dataset within the loop\n",
    "#     rehearsal_dataset = load_rehearsal_data(rehearsal_dir, data_transforms)\n",
    "    \n",
    "#     # Initialize the rehearsal buffer with the updated datasets\n",
    "#     rehearsal_buffer = RehearsalBuffer(rehearsal_dataset)\n",
    "\n",
    "#     # Get the first new class\n",
    "#     first_new_class = test_dataset.classes[i]\n",
    "\n",
    "#     # Function to update the model for new classes\n",
    "#     def add_new_classes_to_model(model, new_classes_count):\n",
    "#         old_num_classes = model.classifier[1].out_features\n",
    "#         new_num_classes = old_num_classes + new_classes_count\n",
    "        \n",
    "#         # Get the weight and bias of the old classifier layer\n",
    "#         old_weight = model.classifier[1].weight.data\n",
    "#         old_bias = model.classifier[1].bias.data\n",
    "        \n",
    "#         # Create a new classifier layer with updated number of classes\n",
    "#         new_classifier = torch.nn.Linear(model.classifier[1].in_features, new_num_classes)\n",
    "        \n",
    "#         # Initialize the new classifier layer with old weights and biases\n",
    "#         new_classifier.weight.data[:old_num_classes] = old_weight\n",
    "#         new_classifier.bias.data[:old_num_classes] = old_bias\n",
    "        \n",
    "#         # Replace the old classifier with the new one\n",
    "#         model.classifier[1] = new_classifier\n",
    "\n",
    "#     # Add new class to the model before starting the training loop\n",
    "#     add_new_classes_to_model(model, 1)  # Increment by one class\n",
    "\n",
    "#     # Iterate over epochs\n",
    "#     for epoch in range(fine_tune_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         # Load data for the new class\n",
    "#         class_index = test_dataset.class_to_idx[first_new_class]\n",
    "#         class_indices = [i for i, (_, label) in enumerate(test_dataset) if label == class_index]\n",
    "#         new_class_subset = Subset(test_dataset, class_indices)\n",
    "#         new_class_loader = DataLoader(new_class_subset, batch_size=64, shuffle=True, num_workers=4)\n",
    "        \n",
    "#         # Wrap the data loader with tqdm for the progress bar\n",
    "#         new_class_loader_with_progress = tqdm(new_class_loader, desc=f'Epoch {epoch + 1}/{fine_tune_epochs} - Class {first_new_class}', leave=False)\n",
    "        \n",
    "#         for images, labels in new_class_loader_with_progress:\n",
    "#             # Adjust labels for the new combined classifier\n",
    "#             labels += 37  # Shift labels to the correct position\n",
    "            \n",
    "#             # Add samples from each class to the rehearsal buffer\n",
    "#             rehearsal_buffer.add_random_data(sample_per_class, sample_per_new_class)\n",
    "            \n",
    "#             # Sample from the rehearsal buffer\n",
    "#             if len(rehearsal_buffer.buffer) > 0:\n",
    "#                 buffer_samples = rehearsal_buffer.sample_data(sample_per_class + sample_per_new_class)\n",
    "#                 buffer_images, buffer_labels = zip(*buffer_samples)\n",
    "#                 buffer_images = torch.stack(buffer_images)\n",
    "#                 buffer_labels = torch.tensor(buffer_labels)\n",
    "                \n",
    "#                 # Combine current batch and buffer samples\n",
    "#                 combined_images = torch.cat((images, buffer_images))\n",
    "#                 combined_labels = torch.cat((labels, buffer_labels))\n",
    "#             else:\n",
    "#                 combined_images = images\n",
    "#                 combined_labels = labels\n",
    "            \n",
    "#             # Zero the parameter gradients\n",
    "#             fine_tune_optimizer.zero_grad()\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(combined_images)\n",
    "#             loss = fine_tune_criterion(outputs, combined_labels)\n",
    "            \n",
    "#             # Backward pass and optimize\n",
    "#             loss.backward()\n",
    "#             fine_tune_optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         # Reduce learning rate if validation loss plateaus\n",
    "#         fine_tune_scheduler.step(running_loss)\n",
    "        \n",
    "#         # Print loss at the end of each epoch\n",
    "#         print(f\"Epoch {epoch + 1}/{fine_tune_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "#     # Copy the new class images to the rehearsal folder at the end of training\n",
    "\n",
    "#     def copy_new_class_to_rehearsal(test_dir, rehearsal_dir, new_class_name, class_index):\n",
    "#         new_class_dir = os.path.join(test_dir, new_class_name)\n",
    "#         rehearsal_class_dir = os.path.join(rehearsal_dir, new_class_name)\n",
    "        \n",
    "#         if not os.path.exists(rehearsal_class_dir):\n",
    "#             os.makedirs(rehearsal_class_dir)\n",
    "        \n",
    "#         for filename in os.listdir(new_class_dir):\n",
    "#             source_path = os.path.join(new_class_dir, filename)\n",
    "#             destination_path = os.path.join(rehearsal_class_dir, filename)\n",
    "#             shutil.copy(source_path, destination_path)\n",
    "        \n",
    "#         # Rename the rehearsal class folder to include the class index\n",
    "#         # new_rehearsal_class_dir = os.path.join(rehearsal_dir, f\"{class_index:02d}_{new_class_name}\")\n",
    "#         # os.rename(rehearsal_class_dir, new_rehearsal_class_dir)\n",
    "\n",
    "\n",
    "#     # Execute the copy function\n",
    "#     copy_new_class_to_rehearsal(test_dir, rehearsal_dir, first_new_class, i + 37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train dataset: 96.56%\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(root='cards_dataset/train', transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "train_accuracy = total_correct / total_samples\n",
    "print(f\"Accuracy on train dataset: {train_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 'ace of diamonds' dataset: 99.22%\n",
      "Average loss on 'ace of diamonds' dataset: 0.1098\n",
      "Accuracy on 'ace of hearts' dataset: 99.42%\n",
      "Average loss on 'ace of hearts' dataset: 0.1030\n",
      "Accuracy on 'ace of spades' dataset: 96.69%\n",
      "Average loss on 'ace of spades' dataset: 0.3991\n",
      "Accuracy on 'eight of clubs' dataset: 97.10%\n",
      "Average loss on 'eight of clubs' dataset: 0.4256\n",
      "Accuracy on 'eight of diamonds' dataset: 76.10%\n",
      "Average loss on 'eight of diamonds' dataset: 1.0330\n",
      "Accuracy on 'eight of hearts' dataset: 98.03%\n",
      "Average loss on 'eight of hearts' dataset: 0.3053\n",
      "Accuracy on 'eight of spades' dataset: 100.00%\n",
      "Average loss on 'eight of spades' dataset: 0.1349\n",
      "Overall accuracy across all classes: 95.02%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directories for the dataset\n",
    "test_dir = 'cards_dataset/test'\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "\n",
    "# Define DataLoader for the entire test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load the trained model\n",
    "model.to(torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "overall_correct = 0\n",
    "overall_counted = 0\n",
    "\n",
    "# Function to evaluate model on a specific class subset\n",
    "def evaluate_class(class_name, class_idx, dataset):\n",
    "    class_indices = [i for i, (_, label) in enumerate(dataset) if label == class_idx]\n",
    "    class_subset = Subset(dataset, class_indices)\n",
    "    class_loader = DataLoader(class_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in class_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels + 37)  # Offset labels by 37\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend((labels + 37).cpu().numpy())  # Offset labels by 37\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == (labels + 37)).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(class_loader)\n",
    "    \n",
    "    return accuracy, average_loss, all_predictions, all_labels\n",
    "\n",
    "# Iterate through each class in the test dataset\n",
    "for i in range(0, 7):\n",
    "    class_name = test_dataset.classes[i]\n",
    "    class_idx = test_dataset.class_to_idx[class_name]\n",
    "    \n",
    "    accuracy_class, avg_loss_class, predictions_class, actuals_class = evaluate_class(class_name, class_idx, test_dataset)\n",
    "    \n",
    "    print(f\"Accuracy on '{class_name}' dataset: {accuracy_class:.2f}%\")\n",
    "    print(f\"Average loss on '{class_name}' dataset: {avg_loss_class:.4f}\")\n",
    "    \n",
    "    # Convert predictions and actual labels to NumPy arrays\n",
    "    predictions_class = np.array(predictions_class)\n",
    "    actuals_class = np.array(actuals_class)\n",
    "    \n",
    "    # Update overall metrics\n",
    "    overall_correct += (predictions_class == actuals_class).sum()\n",
    "    overall_counted += len(actuals_class)\n",
    "    \n",
    "    # # Print predictions along with actual labels for the current class subset\n",
    "    # print(f\"Predictions on '{class_name}' dataset:\")\n",
    "    # for idx, (pred, actual) in enumerate(zip(predictions_class, actuals_class)):\n",
    "    #     print(f\"Sample {idx}: Predicted class {pred}, Actual class {actual}\")\n",
    "\n",
    "# Print overall accuracy across all classes\n",
    "overall_accuracy = 100 * overall_correct / overall_counted\n",
    "print(f\"Overall accuracy across all classes: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, average_loss, all_predictions, all_labels\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the \"ace of diamonds\" subset\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m accuracy_class, avg_loss_class, predictions_class, actuals_class \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune_criterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mace of diamonds\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_class\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage loss on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mace of diamonds\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss_class\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[82], line 39\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, data_loader, criterion)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m---> 39\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     41\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchvision\\models\\efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\activation.py:396\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py:2101\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Subset, DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Define the data transformations\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Directories for the dataset\n",
    "# train_dir = 'cards_dataset/test'\n",
    "# train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "\n",
    "# # Get the class index for \"\"\n",
    "# class_name = \"ace of diamonds\"\n",
    "# class_idx = train_dataset.class_to_idx[class_name]\n",
    "\n",
    "# # Filter indices for the \"two of spades\" class\n",
    "# class_indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_idx]\n",
    "\n",
    "# # Create a subset and DataLoader for the \"two of spades\" class\n",
    "# class_subset = Subset(train_dataset, class_indices)\n",
    "# class_loader = DataLoader(class_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# # Define the evaluation function\n",
    "# def evaluate_model(model, data_loader, criterion):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     total_loss = 0.0\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in data_loader:\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == 37).sum().item()\n",
    "    \n",
    "#     accuracy = 100 * correct / total\n",
    "#     average_loss = total_loss / len(data_loader)\n",
    "#     return accuracy, average_loss, all_predictions, all_labels\n",
    "\n",
    "# # Evaluate the model on the \"ace of diamonds\" subset\n",
    "# accuracy_class, avg_loss_class, predictions_class, actuals_class = evaluate_model(model, class_loader, fine_tune_criterion)\n",
    "\n",
    "# print(f\"Accuracy on 'ace of diamonds' dataset: {accuracy_class:.2f}%\")\n",
    "# print(f\"Average loss on 'ace of diamonds' dataset: {avg_loss_class:.4f}\")\n",
    "\n",
    "# # Print predictions along with actual labels for the \"two of spades\" dataset\n",
    "# print(\"Predictions on 'ace of diamonds' dataset:\")\n",
    "# for idx, (pred, actual) in enumerate(zip(predictions_class, actuals_class)):\n",
    "#     print(f\"Sample {idx}: Predicted class {pred}, Actual class {actual + 37}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 'ace of hearts' dataset: 90.06%\n",
      "Average loss on 'ace of hearts' dataset: 11.1137\n",
      "Predictions on 'ace of hearts' dataset:\n",
      "Sample 0: Predicted class 38, Actual class 38\n",
      "Sample 1: Predicted class 38, Actual class 38\n",
      "Sample 2: Predicted class 38, Actual class 38\n",
      "Sample 3: Predicted class 38, Actual class 38\n",
      "Sample 4: Predicted class 38, Actual class 38\n",
      "Sample 5: Predicted class 38, Actual class 38\n",
      "Sample 6: Predicted class 38, Actual class 38\n",
      "Sample 7: Predicted class 38, Actual class 38\n",
      "Sample 8: Predicted class 38, Actual class 38\n",
      "Sample 9: Predicted class 38, Actual class 38\n",
      "Sample 10: Predicted class 37, Actual class 38\n",
      "Sample 11: Predicted class 38, Actual class 38\n",
      "Sample 12: Predicted class 38, Actual class 38\n",
      "Sample 13: Predicted class 38, Actual class 38\n",
      "Sample 14: Predicted class 38, Actual class 38\n",
      "Sample 15: Predicted class 38, Actual class 38\n",
      "Sample 16: Predicted class 38, Actual class 38\n",
      "Sample 17: Predicted class 38, Actual class 38\n",
      "Sample 18: Predicted class 38, Actual class 38\n",
      "Sample 19: Predicted class 38, Actual class 38\n",
      "Sample 20: Predicted class 38, Actual class 38\n",
      "Sample 21: Predicted class 38, Actual class 38\n",
      "Sample 22: Predicted class 38, Actual class 38\n",
      "Sample 23: Predicted class 37, Actual class 38\n",
      "Sample 24: Predicted class 38, Actual class 38\n",
      "Sample 25: Predicted class 38, Actual class 38\n",
      "Sample 26: Predicted class 38, Actual class 38\n",
      "Sample 27: Predicted class 38, Actual class 38\n",
      "Sample 28: Predicted class 37, Actual class 38\n",
      "Sample 29: Predicted class 38, Actual class 38\n",
      "Sample 30: Predicted class 38, Actual class 38\n",
      "Sample 31: Predicted class 38, Actual class 38\n",
      "Sample 32: Predicted class 38, Actual class 38\n",
      "Sample 33: Predicted class 38, Actual class 38\n",
      "Sample 34: Predicted class 38, Actual class 38\n",
      "Sample 35: Predicted class 38, Actual class 38\n",
      "Sample 36: Predicted class 38, Actual class 38\n",
      "Sample 37: Predicted class 38, Actual class 38\n",
      "Sample 38: Predicted class 38, Actual class 38\n",
      "Sample 39: Predicted class 38, Actual class 38\n",
      "Sample 40: Predicted class 38, Actual class 38\n",
      "Sample 41: Predicted class 38, Actual class 38\n",
      "Sample 42: Predicted class 38, Actual class 38\n",
      "Sample 43: Predicted class 37, Actual class 38\n",
      "Sample 44: Predicted class 38, Actual class 38\n",
      "Sample 45: Predicted class 38, Actual class 38\n",
      "Sample 46: Predicted class 38, Actual class 38\n",
      "Sample 47: Predicted class 38, Actual class 38\n",
      "Sample 48: Predicted class 38, Actual class 38\n",
      "Sample 49: Predicted class 38, Actual class 38\n",
      "Sample 50: Predicted class 37, Actual class 38\n",
      "Sample 51: Predicted class 38, Actual class 38\n",
      "Sample 52: Predicted class 38, Actual class 38\n",
      "Sample 53: Predicted class 38, Actual class 38\n",
      "Sample 54: Predicted class 38, Actual class 38\n",
      "Sample 55: Predicted class 37, Actual class 38\n",
      "Sample 56: Predicted class 38, Actual class 38\n",
      "Sample 57: Predicted class 38, Actual class 38\n",
      "Sample 58: Predicted class 38, Actual class 38\n",
      "Sample 59: Predicted class 38, Actual class 38\n",
      "Sample 60: Predicted class 38, Actual class 38\n",
      "Sample 61: Predicted class 38, Actual class 38\n",
      "Sample 62: Predicted class 38, Actual class 38\n",
      "Sample 63: Predicted class 38, Actual class 38\n",
      "Sample 64: Predicted class 38, Actual class 38\n",
      "Sample 65: Predicted class 38, Actual class 38\n",
      "Sample 66: Predicted class 38, Actual class 38\n",
      "Sample 67: Predicted class 38, Actual class 38\n",
      "Sample 68: Predicted class 38, Actual class 38\n",
      "Sample 69: Predicted class 38, Actual class 38\n",
      "Sample 70: Predicted class 37, Actual class 38\n",
      "Sample 71: Predicted class 38, Actual class 38\n",
      "Sample 72: Predicted class 38, Actual class 38\n",
      "Sample 73: Predicted class 38, Actual class 38\n",
      "Sample 74: Predicted class 38, Actual class 38\n",
      "Sample 75: Predicted class 38, Actual class 38\n",
      "Sample 76: Predicted class 38, Actual class 38\n",
      "Sample 77: Predicted class 38, Actual class 38\n",
      "Sample 78: Predicted class 38, Actual class 38\n",
      "Sample 79: Predicted class 38, Actual class 38\n",
      "Sample 80: Predicted class 38, Actual class 38\n",
      "Sample 81: Predicted class 37, Actual class 38\n",
      "Sample 82: Predicted class 38, Actual class 38\n",
      "Sample 83: Predicted class 38, Actual class 38\n",
      "Sample 84: Predicted class 38, Actual class 38\n",
      "Sample 85: Predicted class 38, Actual class 38\n",
      "Sample 86: Predicted class 38, Actual class 38\n",
      "Sample 87: Predicted class 38, Actual class 38\n",
      "Sample 88: Predicted class 38, Actual class 38\n",
      "Sample 89: Predicted class 38, Actual class 38\n",
      "Sample 90: Predicted class 38, Actual class 38\n",
      "Sample 91: Predicted class 38, Actual class 38\n",
      "Sample 92: Predicted class 38, Actual class 38\n",
      "Sample 93: Predicted class 38, Actual class 38\n",
      "Sample 94: Predicted class 38, Actual class 38\n",
      "Sample 95: Predicted class 38, Actual class 38\n",
      "Sample 96: Predicted class 38, Actual class 38\n",
      "Sample 97: Predicted class 38, Actual class 38\n",
      "Sample 98: Predicted class 38, Actual class 38\n",
      "Sample 99: Predicted class 38, Actual class 38\n",
      "Sample 100: Predicted class 38, Actual class 38\n",
      "Sample 101: Predicted class 38, Actual class 38\n",
      "Sample 102: Predicted class 38, Actual class 38\n",
      "Sample 103: Predicted class 38, Actual class 38\n",
      "Sample 104: Predicted class 38, Actual class 38\n",
      "Sample 105: Predicted class 38, Actual class 38\n",
      "Sample 106: Predicted class 38, Actual class 38\n",
      "Sample 107: Predicted class 38, Actual class 38\n",
      "Sample 108: Predicted class 38, Actual class 38\n",
      "Sample 109: Predicted class 38, Actual class 38\n",
      "Sample 110: Predicted class 38, Actual class 38\n",
      "Sample 111: Predicted class 38, Actual class 38\n",
      "Sample 112: Predicted class 38, Actual class 38\n",
      "Sample 113: Predicted class 38, Actual class 38\n",
      "Sample 114: Predicted class 38, Actual class 38\n",
      "Sample 115: Predicted class 38, Actual class 38\n",
      "Sample 116: Predicted class 38, Actual class 38\n",
      "Sample 117: Predicted class 38, Actual class 38\n",
      "Sample 118: Predicted class 37, Actual class 38\n",
      "Sample 119: Predicted class 38, Actual class 38\n",
      "Sample 120: Predicted class 38, Actual class 38\n",
      "Sample 121: Predicted class 38, Actual class 38\n",
      "Sample 122: Predicted class 37, Actual class 38\n",
      "Sample 123: Predicted class 38, Actual class 38\n",
      "Sample 124: Predicted class 38, Actual class 38\n",
      "Sample 125: Predicted class 38, Actual class 38\n",
      "Sample 126: Predicted class 38, Actual class 38\n",
      "Sample 127: Predicted class 38, Actual class 38\n",
      "Sample 128: Predicted class 38, Actual class 38\n",
      "Sample 129: Predicted class 37, Actual class 38\n",
      "Sample 130: Predicted class 38, Actual class 38\n",
      "Sample 131: Predicted class 38, Actual class 38\n",
      "Sample 132: Predicted class 38, Actual class 38\n",
      "Sample 133: Predicted class 38, Actual class 38\n",
      "Sample 134: Predicted class 38, Actual class 38\n",
      "Sample 135: Predicted class 38, Actual class 38\n",
      "Sample 136: Predicted class 38, Actual class 38\n",
      "Sample 137: Predicted class 38, Actual class 38\n",
      "Sample 138: Predicted class 38, Actual class 38\n",
      "Sample 139: Predicted class 38, Actual class 38\n",
      "Sample 140: Predicted class 38, Actual class 38\n",
      "Sample 141: Predicted class 37, Actual class 38\n",
      "Sample 142: Predicted class 38, Actual class 38\n",
      "Sample 143: Predicted class 38, Actual class 38\n",
      "Sample 144: Predicted class 38, Actual class 38\n",
      "Sample 145: Predicted class 37, Actual class 38\n",
      "Sample 146: Predicted class 38, Actual class 38\n",
      "Sample 147: Predicted class 38, Actual class 38\n",
      "Sample 148: Predicted class 37, Actual class 38\n",
      "Sample 149: Predicted class 38, Actual class 38\n",
      "Sample 150: Predicted class 38, Actual class 38\n",
      "Sample 151: Predicted class 38, Actual class 38\n",
      "Sample 152: Predicted class 38, Actual class 38\n",
      "Sample 153: Predicted class 38, Actual class 38\n",
      "Sample 154: Predicted class 38, Actual class 38\n",
      "Sample 155: Predicted class 38, Actual class 38\n",
      "Sample 156: Predicted class 38, Actual class 38\n",
      "Sample 157: Predicted class 38, Actual class 38\n",
      "Sample 158: Predicted class 38, Actual class 38\n",
      "Sample 159: Predicted class 37, Actual class 38\n",
      "Sample 160: Predicted class 38, Actual class 38\n",
      "Sample 161: Predicted class 38, Actual class 38\n",
      "Sample 162: Predicted class 38, Actual class 38\n",
      "Sample 163: Predicted class 38, Actual class 38\n",
      "Sample 164: Predicted class 38, Actual class 38\n",
      "Sample 165: Predicted class 38, Actual class 38\n",
      "Sample 166: Predicted class 38, Actual class 38\n",
      "Sample 167: Predicted class 38, Actual class 38\n",
      "Sample 168: Predicted class 37, Actual class 38\n",
      "Sample 169: Predicted class 37, Actual class 38\n",
      "Sample 170: Predicted class 38, Actual class 38\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Subset, DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Define the data transformations\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Directories for the dataset\n",
    "# train_dir = 'cards_dataset/test'\n",
    "# train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "\n",
    "# # Get the class index for \"\"\n",
    "# class_name = \"ace of hearts\"\n",
    "# class_idx = train_dataset.class_to_idx[class_name]\n",
    "\n",
    "# # Filter indices for the \"two of spades\" class\n",
    "# class_indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_idx]\n",
    "\n",
    "# # Create a subset and DataLoader for the \"two of spades\" class\n",
    "# class_subset = Subset(train_dataset, class_indices)\n",
    "# class_loader = DataLoader(class_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# # Define the evaluation function\n",
    "# def evaluate_model(model, data_loader, criterion):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     total_loss = 0.0\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in data_loader:\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == 38).sum().item()\n",
    "    \n",
    "#     accuracy = 100 * correct / total\n",
    "#     average_loss = total_loss / len(data_loader)\n",
    "#     return accuracy, average_loss, all_predictions, all_labels\n",
    "\n",
    "# # Evaluate the model on the \"ace of diamonds\" subset\n",
    "# accuracy_class, avg_loss_class, predictions_class, actuals_class = evaluate_model(model, class_loader, fine_tune_criterion)\n",
    "\n",
    "# print(f\"Accuracy on 'ace of hearts' dataset: {accuracy_class:.2f}%\")\n",
    "# print(f\"Average loss on 'ace of hearts' dataset: {avg_loss_class:.4f}\")\n",
    "\n",
    "# # Print predictions along with actual labels for the \"two of spades\" dataset\n",
    "# print(\"Predictions on 'ace of hearts' dataset:\")\n",
    "# for idx, (pred, actual) in enumerate(zip(predictions_class, actuals_class)):\n",
    "#     print(f\"Sample {idx}: Predicted class {pred}, Actual class {actual + 37}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
